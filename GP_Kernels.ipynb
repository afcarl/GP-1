{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kernels\n",
    "\n",
    " \n",
    "$$ \\log p(\\mathbf{y} \\mid X, \\theta) = - \\frac{1}{2} \\mathbf{y}^\\top K_y^{-1}\\mathbf{y} - \\frac{1}{2} \\log \\lvert K \\rvert - \\frac{n}{2} \\log 2 \\pi $$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\log p(\\mathbf{y} \\mid X, \\theta)}{\\partial \\theta_j} = \\frac{1}{2} \\mathbf{y}^\\top K^{-1} \\frac{K}{\\partial \\theta_j} K^{-1} \\mathbf{y} - \\frac{1}{2} \\text{tr}   \\left(K^{-1} \\frac{K}{\\partial \\theta_j} \\right) \\\\\n",
    "= \\frac{1}{2} \\text{tr} \\left( (\\alpha \\alpha^\\top - K^{-1})\\frac{K}{\\partial \\theta_j} \\right)\n",
    "$$\n",
    "\n",
    "With $\\alpha = K^{-1}\\mathbf{y}$\n",
    "\n",
    "TODO: what about mean: http://is.tuebingen.mpg.de/fileadmin/user_upload/files/publications/pdf2903.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$K(x,y) = e^{\\textstyle \\left( -\\frac{1}{2\\sigma^2} \\lVert x-y \\rVert^2 \\right)} $$\n",
    "\n",
    "$$\n",
    "\\frac{K}{\\partial \\sigma} = \\frac{\\lVert x-y \\rVert^2}{\\sigma^3} e^{\\textstyle \\left( -\\frac{1}{2\\sigma^2} \\lVert x-y \\rVert^2 \\right)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm # Colormaps\n",
    "import matplotlib.gridspec as gridspec\n",
    "import scipy.spatial\n",
    "# Allow matplotlib to plot inside this notebook\n",
    "%matplotlib inline\n",
    "# Set the seed of the numpy random number generator so that the tutorial is reproducable\n",
    "np.random.seed(seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Define the RBF kernel\n",
    "def rbf_kernel(x, y, width):\n",
    "    \"\"\"Radial Basis Function (RBF) kernel\"\"\"\n",
    "    sqNorm = np.square(x) + np.square(y).T - 2*np.dot(x, y.T)  # (a-b)^2 = a^2 + b^2 - 2ab\n",
    "    return np.exp(-(1.0/(2*width**2) * sqNorm)) #+ 1e-14 * np.eye(n1)\n",
    "\n",
    "# Define the RBF kernel\n",
    "def rbf_deriv(x, y, width):\n",
    "    \"\"\"Radial Basis Function (RBF) kernel\"\"\"\n",
    "    sqNorm = np.square(x) + np.square(y).T - 2*np.dot(x, y.T)  # (a-b)^2 = a^2 + b^2 - 2ab\n",
    "    return (sqNorm/(width**3)) * np.exp(-(1.0/(2*width**2) * sqNorm)) #+ 1e-14 * np.eye(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 [[-0.99573594]\n",
      " [ 2.64389392]\n",
      " [-5.9986275 ]]\n",
      "y1 [[-0.83915946]\n",
      " [ 0.47740472]\n",
      " [ 0.28073307]]\n"
     ]
    }
   ],
   "source": [
    "# Compute the posterior mean and covariance, and sample some functions.\n",
    "# Define the true function that we want to regress on\n",
    "f = lambda x: np.sin(x)\n",
    "\n",
    "n1 = 3  # Number of points to condition on (training points)\n",
    "width = 1  # The width of the RBF kernel\n",
    "domain = (-6, 6)\n",
    "\n",
    "# Sample some points (X1, y1) on the function\n",
    "X1 = np.random.uniform(domain[0], domain[1], size=(n1,1))\n",
    "y1 = f(X1)\n",
    "\n",
    "print 'X1', X1\n",
    "print 'y1', y1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXMAAAEACAYAAABBDJb9AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n8H/078OzwWkIYQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1034aad90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(X1, y1, 'bo', linewidth=2)\n",
    "plt.axis([domain[0], domain[1], -3, 3])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "eps = 0.0001\n",
    "param = 1\n",
    "grad_func = rbf_deriv(X1, X1, param)\n",
    "plus_kernel = rbf_kernel(X1, X1, param + eps)\n",
    "min_kernel = rbf_kernel(X1, X1, param - eps)\n",
    "# calculate numerical gradient\n",
    "grad_num = (plus_kernel - min_kernel)/(2*eps)\n",
    "#print 'grad_num', grad_num\n",
    "#print 'grad_func', grad_func\n",
    "# Raise error if the numerical grade is not close to the backprop gradient\n",
    "if not np.all(np.isclose(grad_num, grad_func)):\n",
    "    raise ValueError('Numerical gradient is not close to the backpropagation gradient!')\n",
    "print('No gradient errors found')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$K = LL^\\top$ means that $K^{-1} = L^{-\\top} L^{-1}$\n",
    "\n",
    "$(AB)^{\\top} = B^{\\top} A^{\\top}$\n",
    "\n",
    "## Posterior\n",
    " \n",
    "$$ \\log p(\\mathbf{y} \\mid X, \\theta) = - \\frac{1}{2} \\mathbf{y}^\\top K_y^{-1}\\mathbf{y} - \\frac{1}{2} \\log \\lvert K \\rvert - \\frac{n}{2} \\log 2 \\pi\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\mathbf{y}^\\top K_y^{-1}\\mathbf{y} = \\mathbf{y}^\\top L^{-\\top} L^{-1} \\mathbf{y} \\\\\n",
    "= (L^{-1} \\mathbf{y})^\\top (L^{-1} \\mathbf{y}) \\\\\n",
    "= \\left(\\frac{\\mathbf{y}}{L^{-1}}\\right)^\\top \\frac{\\mathbf{y}}{L^{-1}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def posterior(y, K):\n",
    "    n = K.shape[0]\n",
    "    #L = np.linalg.cholesky(K)\n",
    "    #yL = np.linalg.solve(L, y)  # y / L\n",
    "    #det = np.linalg.det(K)\n",
    "    #return -(1/2) * yL.T.dot(yL) -(1/2) * np.log(det) - (n/2) * np.log(2*np.pi)\n",
    "    return -(1/2) * y.T.dot(np.linalg.inv(K)).dot(y) - (1/2) * np.log(np.linalg.det(K)) - (n/2) * np.log(2*np.pi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Posterior Grad\n",
    " \n",
    "$$\n",
    "\\frac{\\partial \\log p(\\mathbf{y} \\mid X, \\theta)}{\\partial \\theta_j} = \\frac{1}{2} \\mathbf{y}^\\top K^{-1} \\frac{K}{\\partial \\theta_j} K^{-1} \\mathbf{y} - \\frac{1}{2} \\text{tr}   \\left(K^{-1} \\frac{K}{\\partial \\theta_j} \\right) \\\\\n",
    "= \\frac{1}{2} \\text{tr} \\left( (\\alpha \\alpha^\\top - K^{-1})\\frac{K}{\\partial \\theta_j} \\right)\n",
    "$$\n",
    "\n",
    "With $\\alpha = K^{-1}\\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def posterior_grad(y, K, Kgrad):\n",
    "    print 'y', y\n",
    "    KInv = np.linalg.inv(K)\n",
    "    a = np.linalg.solve(K, y)  # y / K\n",
    "    #a = KInv.dot(y) \n",
    "    aaK = a.dot(a.T)-KInv\n",
    "    print 'aaK', aaK\n",
    "    #print 'Kgrad', Kgrad\n",
    "    return (1/2) * np.trace(aaK.dot(Kgrad))\n",
    "    #return (1/2) * y.T.dot(KInv).dot(Kgrad).dot(KInv).dot(y) - (1/2)*np.trace(KInv.dot(Kgrad))\n",
    "    #print 'np.asmatrix(y.dot(y.T))', np.asmatrix(y.dot(y.T))\n",
    "    #return (1/2) * np.trace(np.asmatrix(y.dot(y.T)).dot(KInv).dot(Kgrad).dot(KInv)) - (1/2)*np.trace(KInv.dot(Kgrad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K.shape (3, 3)\n",
      "Kgrad.shape (3, 3)\n",
      "y [[-0.83915946]\n",
      " [ 0.47740472]\n",
      " [ 0.28073307]]\n",
      "aaK [[-0.29474383 -0.40053108 -0.23575753]\n",
      " [-0.40053108 -0.77101973  0.13433805]\n",
      " [-0.23575753  0.13433805 -0.92118721]]\n",
      "grad_num_post [[-0.0070722]]\n",
      "grad_func_post -0.00707220285046\n",
      "No gradient errors found\n"
     ]
    }
   ],
   "source": [
    "eps = 0.0001\n",
    "param = 1\n",
    "K = rbf_kernel(X1, X1, param)\n",
    "Kgrad = rbf_deriv(X1, X1, param)\n",
    "print 'K.shape', K.shape\n",
    "print 'Kgrad.shape', Kgrad.shape\n",
    "\n",
    "grad_func_post = posterior_grad(y1, K, Kgrad)\n",
    "plus_post = posterior(y1, rbf_kernel(X1, X1, param+eps))\n",
    "min_post = posterior(y1, rbf_kernel(X1, X1, param-eps))\n",
    "# calculate numerical gradient\n",
    "grad_num_post = (plus_post - min_post)/(2*eps)\n",
    "print 'grad_num_post', grad_num_post\n",
    "print 'grad_func_post', grad_func_post\n",
    "# Raise error if the numerical grade is not close to the backprop gradient\n",
    "if not np.isclose(grad_num_post, grad_func_post):\n",
    "    raise ValueError('Numerical gradient is not close to the backpropagation gradient!')\n",
    "print('No gradient errors found')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
